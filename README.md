# Course Information
https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=114&Sem=1&CrsNo=535361&lang=zh-tw
- Prerequisite: Probability, linear algebra, machine learning
- Textbooks: Goodfellow and Y. Bengio and A. Courville, Deep Learning, The MIT Press, 2016
- Homework and Assignments, Exams and Quizzes, Evaluation and Grading Policy
1. Homework (programming): 30%
2. Paper presentation: 15%
3. Midterm exam: 30%
4. Final project: 25%

# Homework
1. Neural Networks from Scratch: Regression, Classification, and Feature Selection
   - Implemented a fully connected neural network using only basic numerical tools (no high-level DL libraries).
   - Built backpropagation + stochastic gradient descent from scratch and applied the model to (1) regression on the Energy Efficiency dataset to predict heating load with RMS evaluation, and (2) binary classification on the Ionosphere dataset using cross-entropy.
   - Designed and tested a feature selection procedure and analyzed how latent features change with different hidden-layer sizes.
2. CNN Image Recognition on MNIST & CIFAR-10 with Architecture and Regularization Analysis
   - Developed convolutional neural networks for image recognition with high-level APIs allowed.
   - Evaluated the impact of stride and filter size, plotted learning curves, and reported train/test accuracy.
   - Visualized weight/bias distributions, examined vs. misclassified examples, and inspected feature maps across convolutional depths.
   - Extended the model with L2 regularization and discussed its effects.
   - Implemented and documented preprocessing steps for CIFAR-10 and repeated the same analysis pipeline.
3. Character-Level Language Modeling with RNN and LSTM
   - Built a character-level language model using standard RNN and LSTM architectures (frameworks allowed).
   - Preprocessed text into one-hot character representations, trained models to minimize bits-per-character (BPC), and compared performance across hidden state sizes and sequence lengths.
   - Showed learning curves and validation results, generated intermediate text samples at multiple training breakpoints, and demonstrated text generation by priming the model with dataset-related words (e.g., Shakespeare-style prompts).

# Final Project
Competition of Kaggle: https://www.kaggle.com/account/login?returnUrl=%2Ft%2F82d5dc69b5c2456095fcb360d53fe431
- Real & Fake (AI) Images Dataset
